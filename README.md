# Dual Perspectives on LLM Agent Performance: Evaluating Agentic Architectures with MCP

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![MCP-Universe](https://img.shields.io/badge/Benchmark-MCP--Universe-green.svg)](https://github.com/SalesforceAIResearch/MCP-Universe)

> **How does agent architecture affect performance when the tool surface is held constant via MCP?**

This repository contains the code, benchmarks, and analysis for our research on comparing single-agent vs. multi-agent architectures for LLM tool use, evaluated on standardized Model Context Protocol (MCP) benchmarks.

<p align="center">
  <img src="docs/architecture_overview.png" alt="Architecture Overview" width="800"/>
</p>

## ğŸ“‹ Table of Contents

- [Key Findings](#-key-findings)
- [Research Questions](#-research-questions)
- [Project Structure](#-project-structure)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Benchmarks](#-benchmarks)
- [Architectures](#-architectures)
- [Results](#-results)
- [Citation](#-citation)
- [Team](#-team)
- [Acknowledgments](#-acknowledgments)

## ğŸ¯ Key Findings

| Finding | Description |
|---------|-------------|
| **Multi-agent matches frontier models** | GPT-OSS 120B multi-agent achieves 42.9% success (same as Gemini 2.5 Pro) with 31Ã— fewer LLM calls |
| **Architecture compensates for model size** | Multi-agent orchestration enables smaller models to match larger single-agent performance |
| **MCP enables fair comparison** | 92.3% tool success rate confirms performance differences are architectural, not tool-related |
| **Exceptional cost efficiency** | CrewAI + Gemma 2 9B achieves 68.2% success at 0.04% of GPT-4.1's cost |

## ğŸ”¬ Research Questions

1. **Which agent architecture demonstrates superior performance across multi-domain MCP tasks?**
2. **Does MCP standardization enable fair architecture comparison?**
3. **Which task types benefit most from multi-agent coordination?**
4. **What is the cost-performance tradeoff for local vs. cloud models?**

## ğŸ“ Project Structure

```
NLP_MCP_AGENTS/
â”œâ”€â”€ MCP-Universe/                    # Forked MCP-Universe benchmark
â”‚   â””â”€â”€ mcpuniverse/
â”‚       â”œâ”€â”€ benchmark/
â”‚       â”‚   â”œâ”€â”€ configs/
â”‚       â”‚   â”‚   â””â”€â”€ test/
â”‚       â”‚   â”‚       â”œâ”€â”€ web_search.yaml
â”‚       â”‚   â”‚       â”œâ”€â”€ location_navigation.yaml
â”‚       â”‚   â”‚       â””â”€â”€ web_search/          # 55 task definitions
â”‚       â”‚   â”œâ”€â”€ runner.py
â”‚       â”‚   â””â”€â”€ report.py
â”‚       â”œâ”€â”€ agents/
â”‚       â”‚   â”œâ”€â”€ web_search_react.py          # Web search orchestrator
â”‚       â”‚   â”œâ”€â”€ query_formulation_agent.py   # Query optimization
â”‚       â”‚   â”œâ”€â”€ search_execution_agent.py    # Search execution
â”‚       â”‚   â”œâ”€â”€ content_fetch_agent.py       # Content retrieval
â”‚       â”‚   â””â”€â”€ fact_verification_agent.py   # Fact verification
â”‚       â””â”€â”€ mcp/
â”‚           â””â”€â”€ servers/
â”‚               â”œâ”€â”€ google_search/           # SerpAPI integration
â”‚               â””â”€â”€ fetch/                   # Content fetching
â”œâ”€â”€ crewai_navigation/               # CrewAI location navigation
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ orchestrator.py
â”‚   â”‚   â”œâ”€â”€ route_planning_agent.py
â”‚   â”‚   â”œâ”€â”€ distance_optimization_agent.py
â”‚   â”‚   â”œâ”€â”€ time_optimization_agent.py
â”‚   â”‚   â””â”€â”€ place_finding_agent.py
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â””â”€â”€ google_maps_tools.py     # 8 MCP-wrapped tools
â”‚   â””â”€â”€ llm_config.py                # Ollama/Gemma configuration
â”œâ”€â”€ results/                         # Benchmark outputs
â”‚   â”œâ”€â”€ web_search/
â”‚   â”‚   â”œâ”€â”€ GPTOSS120B_multiagent.md
â”‚   â”‚   â”œâ”€â”€ GPTOSS20B_single.md
â”‚   â”‚   â””â”€â”€ Gemini2.5_Pro.md
â”‚   â””â”€â”€ location_navigation/
â”‚       â””â”€â”€ crewai_gemma2_9b.md
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ WEB_SEARCH_ARCHITECTURE.md
â”‚   â”œâ”€â”€ LOCATION_NAVIGATION_ARCHITECTURE.md
â”‚   â””â”€â”€ figures/
â””â”€â”€ paper/
    â”œâ”€â”€ acl_paper.tex
    â””â”€â”€ acl_paper.pdf
```

## ğŸ›  Installation

### Prerequisites

- Python 3.11+
- [Ollama](https://ollama.ai/) (for local model inference)
- Google Cloud API key (for Maps API)
- SerpAPI key (for web search)

### Setup

```bash
# Clone the repository
git clone https://github.com/prasad-yashdeep/NLP_MCP_AGENTS.git
cd NLP_MCP_AGENTS

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install MCP-Universe
cd MCP-Universe
pip install -e .
cd ..

# Install CrewAI components
pip install crewai==0.41.1 crewai-tools

# Pull Ollama models (for local inference)
ollama pull gemma2:9b
ollama pull gemma3:27b
```

### Environment Variables

Create a `.env` file in the project root:

```bash
# API Keys
SERPAPI_KEY=your_serpapi_key
GOOGLE_MAPS_API_KEY=your_google_maps_key
OPENROUTER_API_KEY=your_openrouter_key  # For GPT-OSS models

# Ollama Configuration
OLLAMA_HOST=http://localhost:11434

# MCP Configuration
MCP_SERVER_TIMEOUT=30
```

## ğŸš€ Quick Start

### Run Web Search Benchmark

```bash
# Single-agent baseline (GPT-OSS 20B)
python -m mcpuniverse.benchmark.runner \
    --config benchmark/configs/test/web_search.yaml \
    --agent single \
    --model gpt-oss-20b

# Multi-agent (GPT-OSS 120B orchestrator)
python -m mcpuniverse.benchmark.runner \
    --config benchmark/configs/test/web_search.yaml \
    --agent multi \
    --model gpt-oss-120b

# Gemini 2.5 Pro baseline
python -m mcpuniverse.benchmark.runner \
    --config benchmark/configs/test/web_search.yaml \
    --agent single \
    --model gemini-2.5-pro
```

### Run Location Navigation Benchmark

```bash
# CrewAI with Gemma 2 9B
cd crewai_navigation
python run_benchmark.py --tasks all --model gemma2:9b

# Run specific task categories
python run_benchmark.py --tasks route_planning --model gemma2:9b
python run_benchmark.py --tasks place_finding --model gemma2:9b
```

### Generate Benchmark Report

```bash
python -m mcpuniverse.benchmark.report \
    --input results/web_search/ \
    --output reports/web_search_summary.md
```

## ğŸ“Š Benchmarks

### Web Search (MCP-Universe)

| Task Category | # Tasks | Description |
|--------------|---------|-------------|
| Factual Information | 17 | Single/multi-fact lookup, statistics |
| Comparison & Analysis | 14 | Entity comparison, rankings, trends |
| Research & Synthesis | 14 | Deep research, topic summarization |
| Current Events | 8 | News retrieval, live data |
| Specialized Search | 2 | Local search, product search |

**Example Task:**
```json
{
  "category": "web_search",
  "question": "What is the current population of Tokyo and its GDP?",
  "mcp_servers": ["google-search", "fetch"],
  "evaluators": [
    {"func": "json -> get(population)", "op": "in_range", "value": [13000000, 14500000]}
  ]
}
```

### Location Navigation (MCP-Universe)

| Task Category | # Tasks | Avg Success | Description |
|--------------|---------|-------------|-------------|
| Place Finding | 11 | 72.8% | Location discovery, coordinate search |
| Time Optimization | 9 | 68.4% | Travel time minimization |
| Route Planning | 10 | 67.5% | Multi-city itineraries |
| Distance Optimization | 10 | 67.1% | Midpoint calculation |
| Multi-modal | 5 | 0% | Complex real-time constraints |

## ğŸ— Architectures

### Web Search Multi-Agent (OpenAI Agents SDK)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           WebSearchOrchestrator (GPT-OSS 120B)              â”‚
â”‚                  ReAct Loop (max 12 iter)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query   â”‚         â”‚ Search  â”‚         â”‚ Content â”‚
â”‚ Agent   â”‚         â”‚ Agent   â”‚         â”‚ Fetch   â”‚
â”‚ (20B)   â”‚         â”‚ (20B)   â”‚         â”‚ (20B)   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚                   â”‚                   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   MCP Servers    â”‚
              â”‚ â€¢ Google Search  â”‚
              â”‚ â€¢ Fetch          â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Location Navigation Multi-Agent (CrewAI)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            CrewAI Orchestrator (Gemma 2 9B)                 â”‚
â”‚     Task Analysis â†’ Agent Selection â†’ Response Synthesis    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼          â–¼          â–¼          â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Route  â”‚ â”‚Distanceâ”‚ â”‚  Time  â”‚ â”‚ Place  â”‚
â”‚Planningâ”‚ â”‚  Opt   â”‚ â”‚  Opt   â”‚ â”‚Finding â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Google Maps MCP    â”‚
         â”‚  (8 tools)          â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ˆ Results

### Web Search Benchmark

| Model (+ Architecture) | Tasks | Success Rate | Avg LLM Calls | Efficiency |
|------------------------|-------|--------------|---------------|------------|
| GPT-OSS 20B (Single) | 7 | **0.0%** | 168.9 | Very Low |
| +Multi-Agent (120B orch.) | 7 | **42.9%** | 5.4 | High |
| Gemini 2.5 Pro (Single) | 7 | **42.9%** | 8.3 | High |

**Key Insight:** Multi-agent achieves same success as Gemini 2.5 Pro with 31Ã— fewer LLM calls than single-agent baseline.

### Location Navigation Benchmark

| Model (+ Framework) | Tasks | Route Plan | Time Opt | Dist Opt | Place Find | Overall |
|---------------------|-------|------------|----------|----------|------------|---------|
| Gemma 2 9B + CrewAI | 45 | 67.5% | 68.4% | 67.1% | 72.8% | **68.2%** |
| GPT-4.1 (baseline) | 45 | 62.5% | 81.1% | 65.2% | 88.8% | **86.7%** |

**Key Insight:** CrewAI achieves 68.2% success at 0.04% of GPT-4.1's cost ($0.05 vs $127.50 per 1K queries).

### MCP Tool Success Rates

| Tool | Calls | Success Rate |
|------|-------|--------------|
| maps_geocode | 234 | 95.7% |
| maps_search_places | 189 | 91.2% |
| maps_directions | 167 | 89.8% |
| maps_distance_matrix | 143 | 93.4% |
| **Average** | **733** | **92.3%** |

## ğŸ“„ Citation

If you use this work, please cite:

```bibtex
@article{prasad2025dual,
  title={Dual Perspectives on LLM Agent Performance: Evaluating Agentic Architectures with MCP},
  author={Prasad, Yashdeep and Shetty, Bhumika Dinesh and Gehani, Ronit and Jagtap, Vedant},
  journal={arXiv preprint},
  year={2025}
}
```

## ğŸ‘¥ Team

| Name | Contribution | Contact |
|------|--------------|---------|
| **Yashdeep Prasad** | MCP-Universe setup, evaluation pipeline, analysis | yp2693@nyu.edu |
| **Bhumika Dinesh Shetty** | AutoGen integration, location/navigation experiments | bds9746@nyu.edu |
| **Ronit Gehani** | Playwright/browser-automation tests, error analysis | rg4881@nyu.edu |
| **Vedant Jagtap** | CrewAI setup, OpenRouter routing, metric collation | vsj7589@nyu.edu |

## ğŸ™ Acknowledgments

- [MCP-Universe](https://github.com/SalesforceAIResearch/MCP-Universe) by Salesforce AI Research
- [CrewAI](https://github.com/joaomdmoura/crewAI) framework
- NYU HPC for computing resources
- NYU NLP research group for feedback

## ğŸ“š References

- Schick et al. (2023). [Toolformer: Language Models Can Learn to Use Tools](https://arxiv.org/abs/2302.04761)
- Yao et al. (2023). [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)
- Hong et al. (2023). [MetaGPT: Meta Programming for Multi-Agent Collaborative Framework](https://arxiv.org/abs/2308.00352)
- Liu et al. (2023). [AgentBench: Evaluating LLMs as Agents](https://arxiv.org/abs/2308.03688)

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<p align="center">
  <b>Built with â¤ï¸ at New York University</b>
</p>
